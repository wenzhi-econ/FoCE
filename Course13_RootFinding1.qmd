---
title: "Course 13, 23, and 25: Root Finding Using Bisection Method and Newton Method"
format:
    html:
        theme: cosmo
        code-fold: false 
        code-line-numbers: true
        code-copy: true
        page-layout: full
        highlight-style: github
        number-sections: true
        toc: true
jupyter: python3
execute: 
    eval: true
    echo: true
---

Python version:

```{python}
# | label: version 
import sys
print(sys.version)
```

In this file, I will summarize some methods to find a root of a function, that is, to solve the equation

$$
f(x) = 0.
$$

# Bisection Method

```{python}
# | label: bisection 

def bisection(f, a=0, b=1, tol=1e-6, maxiter=100, callback=None):
    """
    This function finds the root of function f (the first argument)
    using the bisection method on the interval [a,b], with given
    tolerance and maximum number of iterations.

    Input arguments:
        f: the function of interest
        a: the lower bound of the interval
        b: the upper bound of the interval
        tol: tolerance
        maxiter: maximum number of iterations
        callback: a function that will be invoked at each iteration if
        given
    """

    if f(a) * f(b) > 0:
        raise ValueError(
            "Function has the same sign at the given bounds."
        )
    for i in range(maxiter):
        err = abs(b - a)
        if err < tol:
            break
        else:
            x = (a + b) / 2
            if f(a) * f(x) > 0:
                a = x
            else:
                b = x
        if callback is not None:
            callback(fun=f, x=x, iter=i)
    else:
        raise RuntimeError(
            "Failed to converge in {maxiter} iterations."
        )
    return x
```

Next, I will test this root-finding function:

```{python}
# | label: bisection example 

f = lambda x: -4 * (x**3) + 5 * x + 1
a, b = -3, -0.5
x = bisection(f, a, b)
print("Solution is x=%1.3f, f(x)=%1.12f" % (x, f(x)))


def show_iteration(fun, x, iter):
    print(f"Iteration {iter:<2d}: fun(x) = {fun(x):<+3.20e}")


x = bisection(f, a, b, callback=show_iteration)
```

# Newton-Raphson (Newton) Method

```{python}
# | label: newton 

def newton(fun, grad, x0, tol=1e-6, maxiter=100, callback=None):
    """
    This finds the root of function fun (the first argument) using the
    newton method, with initial guess x0 and given tolerance and maximum
    number of iterations.

    Input arguments:
        fun: function of interest
        grad: the first derivative of func
        x0: initial guess
        tol: tolerance
        maxiter: maximum number of iterations
        callback: a function that will be invoked at each iteration if
        given
    """
    for i in range(maxiter):
        x1 = x0 - fun(x0) / grad(x0)
        x_bar = (x1 + x0) / 2
        err = fun(x_bar)

        if callback is not None:
            callback(
                cb_arg_fun=fun,
                cb_arg_grad=grad,
                cb_arg_x0=x0,
                cb_arg_x1=x1,
                cb_arg_x_bar=x_bar,
                cb_arg_iter=i,
            )

        if abs(err) < tol:
            break
        else:
            x0 = x1
    else:
        raise RuntimeError(f"Failed to converge in {maxiter} iterations.")
    return x_bar
```

Then, let's test this function using a simple polynomial function.

```{python}
# | label: newton test 1

f = lambda x: -4 * (x**3) + 5 * x + 1
g = lambda x: -12 * (x**2) + 5
x = newton(f, g, x0=-2.5, maxiter=7)
print(f"Solution is {x=:1.20f}, {f(x)=:1.20f}.")
```

To show the iteration process, let's create a `print_step` function which will be called in each iteration.

```{python}
# | label: newtwon test 2

def print_step(
    cb_arg_fun, cb_arg_grad, cb_arg_x_bar, cb_arg_iter, **kwargs
):
    print(
        f"Iteration {cb_arg_iter+1:<1d}: \nx_bar={cb_arg_x_bar:<1.20f}\nfunction value at x_bar={cb_arg_fun(cb_arg_x_bar):<1.20f}\ngradient at x_bar={cb_arg_grad(cb_arg_x_bar):<1.20f}\n"
    )


x_bar = newton(f, g, x0=-2.5, callback=print_step)
```

A better way is to visualize the iteration process:

```{python}
# | label: newton test 3

import numpy as np
import matplotlib.pyplot as plt

a, b = -3, -0.5  
xd = np.linspace(a, b, 1000)  

def plot_step(
    cb_arg_fun, cb_arg_grad, cb_arg_x0, cb_arg_x1, cb_arg_iter, **kwargs
):
    plot_step.counter += 1
    if cb_arg_iter < 10:
        plt.plot(xd, cb_arg_fun(xd), c="red")
        plt.plot([a, b], [0, 0], c="black")
        ylim = [min(cb_arg_fun(b), 0), cb_arg_fun(a)]
        plt.plot([cb_arg_x0, cb_arg_x0], ylim, c="grey")
        l = lambda z: cb_arg_grad(cb_arg_x0) * (z - cb_arg_x1)
        plt.plot(xd, l(xd), c="green")
        plt.ylim(bottom=10 * cb_arg_fun(b))
        plt.title(f"Iteration {cb_arg_iter + 1}")
        plt.ion()
        plt.show()

plot_step.counter = 0 
newton(f, g, x0=-2.5, callback=plot_step)
print(f"Converged in {plot_step.counter} steps" )
```

# Problems with the Newton Method

## A graphical illustration

First, I will write down codes to graphically illustrate the iteration of the Newton method, and present a well-functioned example.

```{python}
# | label: illustration

def newton_pic(f, g, x0, a=0, b=1, **kwargs):
    '''
    This function draws the iteration of the Newton method in one graph,
    with bounds [a,b].
    '''
    xd = np.linspace(start=a, stop=b, num=1000)
    plt.plot(xd, f(xd), c='red')
    plt.plot([a, b], [0, 0], c='black')
    ylim = [f(a), min(f(b), 0)]
    def plot_step_inner(**kwargs):
        plot_step_inner.counter += 1
        x0 = kwargs['cb_arg_x0']
        x1 = kwargs['cb_arg_x1']
        f0 = kwargs['cb_arg_fun'](x0) 
        plt.plot([x0,x0], [0,f0], c='green')
        plt.plot([x0,x1], [f0,0], c='green')
    plot_step_inner.counter = 0 

    try:
        x_bar = newton(f, g, x0, callback=plot_step_inner, **kwargs)
        print(f'Converged in {plot_step_inner.counter} steps.')
        print(f'{x_bar =:<1.20f}, function value is {f(x_bar):<1.20f}.')
    except RuntimeError:
        print(f'Failed to converge in {plot_step_inner.counter} iterations.')
    
    plt.xlim((a, b))
    plt.ion()
    plt.show()

#!! good case
f_test = lambda x: -4 * (x**3) + 5 * x + 1
g_test = lambda x: -12 * (x**2) + 5

newton_pic(f=f_test, g=g_test, x0=-2.5 , a=-3, b=-0.5, maxiter=10)
```

## Multiple solutions and sensitivity to the initial guess

```{python}
# | label: multiple 1
newton_pic(f=f_test, g=g_test, x0=0.2, a=-2, b=1.5, maxiter=10)
```

```{python}
# | label: multiple 2
newton_pic(f=f_test, g=g_test, x0=1.0, a=-2, b=1.5, maxiter=10)
```

## Diversion and cycles

```{python}
# | label: diversion 
f = lambda x: np.arctan(x)
g = lambda x: 1 / (1 + x**2)
newton_pic(f=f, g=g, x0=1.25, a=-20, b=20)
newton_pic(f=f, g=g, x0=1.5, a=-20, b=20, maxiter=10)
```

```{python}
# | label: cycle 
f = lambda x: -4 * x**3 + 5 * x + 1
g = lambda x: -12 * x**2 + 5
x0 = -0.5689842546416416
newton_pic(f, g, x0, a=-1.5, b=1.5, maxiter=15)
```

## Function domain and differentiability

```{python}
# | label: differentiability 


f = lambda x: np.log(x)
g = lambda x: 1/x
x0 = 2.9
newton_pic(f,g,x0,a=0.001,b=3)
```

# Multivariate Newton Method

Here, let's consider a multivariate root-finding problem given by

$$
G(x) = 0,
$$

where $G(x)$ is a $n$-valued function, and $x \in \mathbb{R}^n$ . The **Jacobian matrix** is the$n$ by $n$ matrix of partial derivatives $\nabla G$. The Newton improvement is given by the following equation,

$$
x_{i+1}=x_i-\left(\nabla G\left(x_i\right)\right)^{-1} G\left(x_i\right).
$$

In this section, let's consider a 2-d case, where function $G$ takes a 2 by 1 column vector as inputs and returns a 2 by 1 column vector, and its Jacobain matrix $H$ returns a 2 by 2 matrix.

$$
\begin{gathered}G(x, y)=\binom{2 \sin (x) \cos (y+\pi)-1.15 \sin (1.25 \pi-2 x)}{2 \cos (x) \sin (y+\pi)} \\H(x, y)=\left(\begin{array}{cc}2 \cos (x) \cos (y+\pi)+2.3 \cos (1.25 \pi-2 x) & -2 \sin (x) \sin (y+\pi) \\-2 \sin (x) \sin (y+\pi) & 2 \cos (x) \cos (y+\pi)\end{array}\right)\end{gathered}.
$$

First, let's code up the newton algorithm. Notice that this function is very specific to our current example, and its arguments should stick to the rules strictly (especially that `x0` should be a column vector as a `numpy.ndarray` object.

```{python}

# | label: 2-d newton method

def newton2(fun, grad, x0, tol=1e-6, maxiter=100, callback=None):
    """
    This finds the root of a 2-dimensional function fun (the first
    argument) using the newton method, with initial guess x0 and given
    tolerance and maximum number of iterations.

    Input arguments:
        fun: a function that receives a 2*1 np.ndarray as input, and
        returns a 2*1 np.ndarray
        grad: the gradient function of func that receives a 2*1 np.ndarray as input, and returns a 2*2 np.ndarray
        x0: initial guess as a 2*1 np.ndarray
        tol: tolerance
        maxiter: maximum number of iterations
        callback: a function that will be invoked at each iteration if
        given
    """
    x, y = x0

    for i in range(maxiter):
        x1 = x0 - np.linalg.inv(grad(x0)).dot(fun(x0))
        x_bar = (x0 + x1) / 2
        err = fun(x_bar)

        if callback is not None:
            callback(
                arg_iter=i,
                arg_err=err,
                arg_xbar=x_bar,
                arg_x0=x0,
                arg_x1=x1,
            )

        if np.sqrt(np.sum(err**2)) < tol:
            break
        else:
            x0 = x1

    else:
        raise RuntimeError(
            f"Failed to converge in {maxiter} iterations."
        )
    return x_bar
```

Now, we can test this function!

```{python}

# | label: test 2-d newton

def g(x_array):
    x = x_array[0, 0]
    y = x_array[1, 0]
    g_1 = 2 * np.sin(x) * np.cos(y + np.pi) \
          - 2 * 0.575 * np.sin(1.25 * np.pi - 2 * x)
    g_2 = 2 * np.cos(x) * np.sin(y + np.pi)
    result = np.array([g_1, g_2]).reshape((2, 1))
    return result


def h(x_array):
    x = x_array[0, 0]
    y = x_array[1, 0]
    h_11 = 2 * np.cos(x) * np.cos(y + np.pi) \
           - 2 * 0.575 * np.sin(1.25 * np.pi - 2 * x)
    h_12 = -2 * np.sin(x) * np.sin(y + np.pi)
    h_21 = -2 * np.sin(x) * np.sin(y + np.pi)
    h_22 = 2 * np.cos(x) * np.cos(y + np.pi)

    result = np.array([[h_11, h_12], [h_21, h_22]])
    return result


x_bar = newton2(g, h, x0=np.array([-1.8, -0.2]).reshape((2, 1)))
print(x_bar)

def print_iter(arg_xbar, arg_err, arg_iter, **kwargs):
    print(f"Iteration {arg_iter+1}")
    print("x_bar =", arg_xbar.ravel())
    print("function value at x_bar =", arg_err.ravel())

x_bar = newton2(g, h, x0=np.array([-1.8, -0.2]).reshape((2, 1)), callback=print_iter)
print(x_bar)
```

# Combining Newton Method with Bisection Method

In this section, we are going to solve the following equation

$$
f(x) = a \log(x) + b\log(1-x) + c = 0, ab<0.
$$

This equation arise in the models of discrete choice in IO, for example when computing a mixed strategy equilibrium in a two players game with binary actions.

## Theoretical properties

-   $x \in (0,1)$ are strict limits, any algorithm will break down if stepping outside
-   there is exactly one solution for any values of parameters $a,b,c$ (where $a$ and $b$ have opposite signs, without loss of generality assume $a > 0$ and $b<0$.)
    -   $f(x)$ is continuous in its domain $x \in (0,1)$.
    -   $f'(x) \ge 0$ everywhere in the domain, so the function is monotonically increasing, and vise versa.
    -   When $x \rightarrow 0$ from the right $f(x) \rightarrow -\infty$, and when $x \rightarrow 1$ from the left $f(x) \rightarrow \infty$, and vise versa.
-   But depending on the value of $c$ the solution may be arbitrary close to $0$ or $1$!
-   Therefore, if we compare newton with bisection:
    -   Newton method can easily overshoot to the outside of domain.
    -   Bisection method may take forever to converge.

## Newton-bisection hybrid algorithm

```{python}
# | label: newton-bisection

def newton_bounds(
    fun,
    grad,
    x0=None,
    bounds=(0, 1),
    tol=1e-6,
    maxiter=100,
    callback=None,
):
    """
    This function combines the newton method and the bisection method to
    solve a root-finding problem with given lower and upper bounds.

    Input arguments:
        fun: the function of interest
        grad: the derivate of the fun
        x0: initial guess
        bounds: a tuple that contains the lower and upper bounds
        tol: tolerance
        maxiter: maximum number of iterations
        callback: a function that will be called in each iteration
    """
    a, b = bounds
    sign_a = np.sign(fun(a))
    sign_b = np.sign(fun(b))
    if sign_a * sign_b > 0:
        raise ValueError(
            "Function has the same signs at the initial lower and upper bounds."
        )

    if x0 is None:
        x0 = (a + b) / 2

    for i in range(maxiter):
        f0 = fun(x0)
        newton_x1 = x0 - fun(x0) / grad(x0)
        if newton_x1 > b or newton_x1 < a:
            if np.sign(f0) * sign_a > 0:
                a = x0
            else:
                b = x0
            x1 = (a + b) / 2
            step_type = "bisection"
        else:
            x1 = newton_x1
            step_type = "newton"

        err = fun(x1)

        if callback is not None:
            callback(
                arg_iter=i,
                arg_type=step_type,
                arg_err=err,
                arg_x0=x0,
                arg_x1=x1,
                arg_bounds=(a, b),
            )

        if abs(err) < tol:
            break

        x0 = x1
    else:
        raise RuntimeError(
            f"Failed to converge in {maxiter} iterations."
        )

    return x1
```

Now, let's present a case where this function works pretty well. But to do this, we need to define two auxiliary functions to present iterations and have a glimpse at the function of interest.

```{python}
# | label: newton bisection auxiliary functions 

def print_iter(
    arg_iter, arg_type, arg_err, arg_x0, arg_x1, arg_bounds, **kwargs
):
    print(f"Iteration {arg_iter+1}: {arg_type:s}")
    print(
        f"x_0 = {arg_x0:<1.20f}, bounds = ({arg_bounds[0]:<1.10f}, {arg_bounds[1]:<1.10f})"
    )
    print(f"x_bar = {arg_x1:<1.20f}, fun(x_bar) = {arg_err:<1.20f}")
    print("\n")
    
def fun_grad_list(a, b, c):
    f = lambda x: a * np.log(x) + b * np.log(1 - x) + c
    g = lambda x: a / x - b / (1 - x)
    return [f, g]
    
def plot_fun(a, b, c):
    fun = fun_grad_list(a, b, c)[0]
    xd = np.linspace(0, 1, 10000)
    plt.plot(xd, fun(xd), c='red')
    plt.plot([0,1], [0,0], c='black')
    plt.ion()
    plt.show()
```

The well-functioned example has the following parameters: `a=1; b=-4; c=0.5`.

```{python}
# | label: newton-bisection test 1

a = 1
b = -4
c = 0.5
plot_fun(a, b, c)

newton_bounds(
    fun_grad_list(a, b, c)[0],
    fun_grad_list(a, b, c)[1],
    bounds=(0, 1),
    callback=print_iter,
    tol=1e-6,
)
```

Another example is as follows.

```{python}
# | label: newton-bisection test 2

a, b, c = 1, -1, -5

plot_fun(a, b, c)
newton_bounds(
    fun_grad_list(a, b, c)[0],
    fun_grad_list(a, b, c)[1],
    x0=0.9,
    bounds=(0, 1),
    callback=print_iter,
    tol=1e-6,
)
```

However, if $c$ is too large, then this function cannot reach its root within a limited number of iterations.

```{python}
# | label: newton-bisection test 3
# | error: true 

np.seterr(all=None, divide='ignore', over=None, under=None, invalid='ignore')

a, b, c = 2, -3, -100

plot_fun(a, b, c)

newton_bounds(
    fun_grad_list(a, b, c)[0],
    fun_grad_list(a, b, c)[1],
    bounds=(0, 1),
    callback=print_iter,
    tol=1e-6,
)
```

Likewise, if $c$ is too small, then this function cannot reach its root within a limited number of iterations. Or in this example, we need to set a larger `maxiter` so that we can find its root (which is extremely close to 0).

```{python}
# | label: newton-bisection test 4
# | error: true 

np.seterr(all=None, divide='ignore', over=None, under=None, invalid='ignore')

a, b, c = 2, -3, 100

plot_fun(a, b, c)

newton_bounds(
    fun_grad_list(a, b, c)[0],
    fun_grad_list(a, b, c)[1],
    bounds=(0, 1),
    callback=print_iter,
    tol=1e-6,
    maxiter=50
)
```

```{python}
# | label: newton-bisection test 4 with a larger maxiter


newton_bounds(
    fun_grad_list(a, b, c)[0],
    fun_grad_list(a, b, c)[1],
    bounds=(0, 1),
    callback=print_iter,
    tol=1e-6,
    maxiter=100
)
```